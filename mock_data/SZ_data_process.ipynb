{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import gensim\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from sklearn import preprocessing\n",
    "from geopandas.tools import sjoin\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dir\n",
    "data_dir = './SZ_data/'\n",
    "# Seeds\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "# Flags\n",
    "poi_exist = True\n",
    "unit_exist = True\n",
    "area_exist = True\n",
    "seq_exist = True\n",
    "model_exist = True\n",
    "flow_exist = True\n",
    "clean_exist = True\n",
    "dis_exist = True\n",
    "adj_exist = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POIs (crs:epsg - 4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_poitype(df):\n",
    "    df_sl = df.copy()\n",
    "    df_sl[[\"s1_1\", \"s1_2\", \"s1_3\", \"s1_4\", \"s1_5\", \"s1_6\"]] = pd.DataFrame(df_sl[\"type\"].str.split(\"|\").to_list())\n",
    "    df_sl[[\"level_1\", \"level_2\", \"level_3\"]] = pd.DataFrame(df_sl[\"s1_1\"].str.split(\";\").to_list(), index=df_sl.index)\n",
    "    df_sl = df_sl[[\"wgslng\", \"wgslat\", \"level_1\", \"level_2\"]].sort_values(\"level_1\")[2:].reset_index(drop=True)\n",
    "    return df_sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GeoDataFrame 1,577,354 * (wgslng, wgslat, level_1, level_2, code, geometry)\n",
    "if not poi_exist:\n",
    "    p_poi = data_dir + 'org_data/SZ_poi_2018.csv'\n",
    "    df_poi = pd.read_csv(p_poi,encoding=\"utf-8\")\n",
    "    df_poi_c = encode_poitype(df_poi)\n",
    "    df_poi_c = df_poi_c[~df_poi_c.duplicated(keep=\"first\")]\n",
    "    le_poi_first_level = preprocessing.LabelEncoder()\n",
    "    le_poi_second_level = preprocessing.LabelEncoder()\n",
    "    df_poi_c['level_1'] = le_poi_first_level.fit_transform(df_poi_c['level_1'].values)\n",
    "    df_poi_c['level_2'] = le_poi_second_level.fit_transform(df_poi_c['level_2'].values)\n",
    "    df_poi_c['code'] = df_poi_c.apply(lambda x: f\"{x['level_1']:.0f}{x['level_2']:.0f}\", axis=1)\n",
    "    gdf_poi = gpd.GeoDataFrame(df_poi_c,geometry=gpd.points_from_xy(df_poi_c['wgslng'],df_poi_c['wgslat'],crs='epsg:4326'))\n",
    "    gdf_poi.to_file(data_dir+'intermediate_results/poi.shp',encoding = 'utf-8')\n",
    "else:\n",
    "    gdf_poi = gpd.read_file(data_dir+'intermediate_results/poi.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial units (crs:epsg - 32650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GeoDataFrame 33,054 * (ID, geometry)\n",
    "if not unit_exist:\n",
    "    p_unit = data_dir + 'org_data/LU_250.shp'\n",
    "    gdf_unit = gpd.read_file(p_unit)\n",
    "    gdf_unit = gdf_unit[~gdf_unit.duplicated(subset=['FID_'],keep=\"first\")]\n",
    "    gdf_unit = gdf_unit[['FID_','geometry']].rename(columns={\"FID_\":\"ID\"})\n",
    "    gdf_unit.to_file(data_dir+'intermediate_results/unit.shp',encoding='utf-8')\n",
    "else:\n",
    "    gdf_unit = gpd.read_file(data_dir+'intermediate_results/unit.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Land use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classremap_dic(src_path,des_path):\n",
    "    df_class = pd.read_excel(src_path,skiprows=2,names = [str(i) for i in range(6)])\n",
    "    df_class[\"5\"] = df_class[\"5\"].fillna(method=\"ffill\")\n",
    "    df_class = df_class.dropna(subset=[\"4\"])\n",
    "    class_mapdict = {x[\"4\"]: x[\"5\"] for _, x in df_class[df_class[\"4\"] != \"/\"][[\"4\", \"5\"]].iterrows()}\n",
    "    class_mapdict[\"特殊用地\"] = \"绿地广场用地\"\n",
    "    class_mapdict[\"水工建筑用地\"] = \"公用设施用地\"\n",
    "    with open(des_path,'wb') as f:\n",
    "        pickle.dump(class_mapdict, f)\n",
    "    return class_mapdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_area(df):\n",
    "    df_area = df.copy()\n",
    "    p_class = data_dir + 'org_data/三调用地分类_更新.xlsx'\n",
    "    des_class = data_dir + 'org_data/class_mapdict.pkl'\n",
    "    class_mapdict = get_classremap_dic(src_path = p_class, des_path = des_class)\n",
    "    df_area[\"CLA\"] = df_area[\"DLMC\"].apply(lambda x: class_mapdict[x])\n",
    "    df_area = df_area.groupby([\"ID\", \"CLA\"]).sum(\"PERCENTAGE\").reset_index()\n",
    "    df_area = df_area.pivot(index=\"ID\", columns=\"CLA\",values=\"PERCENTAGE\").fillna(0)\n",
    "    return df_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame 33,054 * (ID, 公共管理与公共服务用地, 公用设施用地, 商业服务设施用地, 居住用地, 工业用地, 绿地广场用地, 道路与交通设施用地, 非建设用地)\n",
    "if not area_exist:\n",
    "    p_lu = data_dir + 'org_data/area_250m_fn.txt'\n",
    "    df_lu = pd.read_csv(p_lu)\n",
    "    df_lu = df_lu.drop(columns=[\"OID_\"]).rename(columns={\"FID_\":\"ID\"})\n",
    "    df_area = pivot_area(df_lu)\n",
    "    df_area = (df_area/100).round(3)\n",
    "    df_area[\"非建设用地\"] = df_area.apply(lambda x: (1-x.drop(\"非建设用地\").sum()), axis=1)\n",
    "    df_area.to_csv(data_dir+'intermediate_results/unit_area.csv')\n",
    "else:\n",
    "    df_area = pd.read_csv(data_dir+'intermediate_results/unit_area.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate sequences & Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequences_by_distancegreedy(gdf_tz, gdf_poi, minpois = 10):\n",
    "    sequences = {}\n",
    "    df_join = sjoin(gdf_poi, gdf_tz, how=\"inner\",op=\"within\")\n",
    "    for tz_ind in tqdm.tqdm(df_join.index_right.unique()):\n",
    "        tz_pois = df_join[df_join.index_right == tz_ind].reset_index()\n",
    "        if tz_pois.shape[0] > minpois:\n",
    "            pnt_num = tz_pois.shape[0]\n",
    "            z = np.array([[complex(g.x, g.y) for g in tz_pois.geometry]])\n",
    "            dismat = abs(z.T-z)\n",
    "            visited = list(np.unravel_index(np.argmax(dismat, axis=None), dismat.shape))\n",
    "            ## list of to be visited points\n",
    "            not_visited = [x for x in range(pnt_num) if x not in visited]\n",
    "            np.random.shuffle(not_visited)\n",
    "            while not_visited:\n",
    "                to_be_visit = not_visited.pop()\n",
    "                if len(visited) == 2:\n",
    "                    visited.insert(1, to_be_visit)\n",
    "                    pass\n",
    "                else:\n",
    "                    search_bound = list(zip(visited[0:-1], visited[1:]))\n",
    "                    dis = [dismat[to_be_visit, x]+dismat[to_be_visit, y] for x, y in search_bound]\n",
    "                    insert_place = dis.index(min(dis))+1\n",
    "                    visited.insert(insert_place, to_be_visit)\n",
    "            sequences[tz_ind] = tz_pois.loc[visited, \"code\"].values\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not seq_exist:\n",
    "    sequences = get_sequences_by_distancegreedy(gdf_unit.set_index('ID').sort_index().to_crs('epsg:4326'),gdf_poi[['code','geometry']])\n",
    "    np.save(data_dir+'intermediate_results/Sequences_greedy.npy',sequences)\n",
    "else:\n",
    "    sequences = np.load(data_dir+'intermediate_results/Sequences_greedy.npy',allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exist:\n",
    "    corpus = [TaggedDocument([str(x) for x in words], [f'd{idx}'])for idx, words in sequences.items()]\n",
    "    model = gensim.models.doc2vec.Doc2Vec(dm=1, vector_size=72, dm_mean=1, window=5, dbow_words=1, min_count=1, epochs=100, seed=1,workers=1)\n",
    "    model.build_vocab(corpus)\n",
    "    model.train(corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    model.save(data_dir+'intermediate_results/doc2vec_model')\n",
    "else:\n",
    "    model = gensim.models.doc2vec.Doc2Vec.load(data_dir+'intermediate_results/doc2vec_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame 12,392 * (index, ZoneVec_1, ZoneVec_2, ...)\n",
    "if not model_exist:\n",
    "    df_zonevec = pd.DataFrame.from_dict({'index':sequences.keys()})\n",
    "    i = 1\n",
    "    for v in model.dv.vectors.T:\n",
    "        df_zonevec[f\"ZoneVec_{i}\"] = v\n",
    "        i += 1\n",
    "    df_zonevec = df_zonevec.set_index(\"index\").sort_index()\n",
    "    df_zonevec.to_csv(data_dir+\"intermediate_results/df_zonevec.csv\",index=True)\n",
    "else:\n",
    "    df_zonevec = pd.read_csv(data_dir+'intermediate_results/df_zonevec.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(dfs):\n",
    "    df = dfs.copy()\n",
    "    df[\"hour\"] = df.period.apply(lambda x: x % 100)\n",
    "    df[\"day\"] = df.date.apply(lambda x: x % 100)\n",
    "    df[\"time\"] = (df[\"day\"]-1)*24+df[\"hour\"]\n",
    "    df_reind = df[[\"o_grid\", \"d_grid\", \"time\",\"cu_pop\"]]\n",
    "    return df_reind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not flow_exist:\n",
    "    p_od = data_dir + 'org_data/cyt_hourod2_sz_201811_250m_test84.csv'\n",
    "    df_od = pd.read_csv(p_od,low_memory=False)\n",
    "    p_map = data_dir + 'org_data/tid_sz_250_wgs84_0.txt'\n",
    "    df_map = pd.read_csv(p_map)\n",
    "    df_map['o_grid'] = df_map['TID'].astype(int)\n",
    "    df_map['d_grid'] = df_map['TID'].astype(int)\n",
    "    df_map['o_id'] = df_map['OID_'].astype(int)\n",
    "    df_map['d_id'] = df_map['OID_'].astype(int)\n",
    "    go_dict = df_map.set_index(['o_grid'])['o_id'].to_dict()\n",
    "    gd_dict = df_map.set_index(['d_grid'])['d_id'].to_dict()\n",
    "    df_od['o_tid'] = df_od['o_grid'].astype(int)\n",
    "    df_od['d_tid'] = df_od['d_grid'].astype(int)\n",
    "    df_od['o_grid'] = df_od['o_tid'].map(go_dict)\n",
    "    df_od['d_grid'] = df_od['d_tid'].map(gd_dict)\n",
    "    df_od = clean_df(df_od)\n",
    "    df_od.to_csv(data_dir+'intermediate_results/total_flow.csv',index=False)\n",
    "else:\n",
    "    df_od = pd.read_csv(data_dir+'intermediate_results/total_flow.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-reminder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.self target - df_area [ID, 公共管理与公共服务用地, 公用设施用地, 商业服务设施用地, 居住用地, 工业用地, 绿地广场用地, 道路与交通设施用地, 非建设用地] 33,054\n",
    "# 2.self property - df_zonevec [index, ZoneVec_1, ...] 12,392 ∈ 33,054 because some regions' pois are below 10.\n",
    "# 3.spatial interaction - df_od [o_grid, d_grid, cu_pop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not clean_exist:\n",
    "    zone_idx = df_zonevec['index'].unique().tolist()\n",
    "    df_od = df_od[df_od[\"o_grid\"].isin(zone_idx)]\n",
    "    df_od = df_od[df_od[\"d_grid\"].isin(zone_idx)]\n",
    "    union_idx = set(df_od.o_grid).union(set(df_od.d_grid))\n",
    "    dic_idx = {x:i for x,i in zip(union_idx,range(len(union_idx)))}\n",
    "    with open(data_dir+'intermediate_results/dic_map.pkl','wb') as f:\n",
    "        pickle.dump(dic_idx,f)\n",
    "    df_zonevec = df_zonevec[df_zonevec['index'].isin(union_idx)]\n",
    "    def zone_remap(dfs,dic_idx):\n",
    "        df = dfs.copy()\n",
    "        df['idx'] = df['index'].map(dic_idx)\n",
    "        return df\n",
    "    df_zonevec = zone_remap(df_zonevec,dic_idx)\n",
    "    df_zonevec.set_index('idx').sort_index().to_csv(data_dir+\"final_input/df_zonevec.csv\")\n",
    "    df_area = df_area[df_area['ID'].isin(union_idx)]\n",
    "    def area_remap(dfs,dic_idx):\n",
    "        df = dfs.copy()\n",
    "        df['idx'] = df[\"ID\"].map(dic_idx)\n",
    "        return df\n",
    "    df_area = area_remap(df_area,dic_idx)\n",
    "    df_area.set_index('idx').sort_index().to_csv(data_dir+'final_input/df_area.csv')\n",
    "    gdf_unit = gdf_unit[gdf_unit[\"ID\"].isin(union_idx)]\n",
    "    def unit_remap(dfs,dic_idx):\n",
    "        df = dfs.copy()\n",
    "        df['idx'] = df['ID'].map(dic_idx)\n",
    "        return df\n",
    "    gdf_unit = unit_remap(gdf_unit,dic_idx)\n",
    "    gdf_unit.set_index('idx').sort_index().to_file(data_dir+'final_input/unit.shp',encoding='utf-8')\n",
    "    def od_remap(dfs,dic_idx):\n",
    "        df = dfs.copy()\n",
    "        df['o_id'] = df['o_grid'].map(dic_idx)\n",
    "        df['d_id'] = df['d_grid'].map(dic_idx)\n",
    "        return df[['o_id','d_id','cu_pop']]\n",
    "    df_od = od_remap(df_od,dic_idx)\n",
    "    flow_mx = np.zeros((len(union_idx),len(union_idx)),dtype=float)\n",
    "    flow_len = df_od.shape[0]\n",
    "    df_od = df_od.reset_index()\n",
    "    for i in range(flow_len):\n",
    "        o_idx = int(df_od.loc[i,'o_id'])\n",
    "        d_idx = int(df_od.loc[i,'d_id'])\n",
    "        cu_pop = float(df_od.loc[i,'cu_pop'])\n",
    "        flow_mx[o_idx,d_idx] = flow_mx[o_idx,d_idx] + cu_pop\n",
    "    flow_mx = flow_mx/30.0\n",
    "    np.save(data_dir+'final_input/flow_mx.npy',flow_mx)\n",
    "else:\n",
    "    df_area = pd.read_csv(data_dir+'final_input/df_area.csv')\n",
    "    df_zonevec = pd.read_csv(data_dir+'final_input/df_zonevec.csv')\n",
    "    gdf_unit = gpd.read_file(data_dir+'final_input/unit.shp')\n",
    "    flow_mx = np.load(data_dir+'final_input/flow_mx.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance-decay & Adjacency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not dis_exist:\n",
    "    df_dis = gdf_unit.geometry.centroid.apply(lambda g: gdf_unit.geometry.centroid.distance(g))\n",
    "    df_dis.to_csv(data_dir+'intermediate_results/df_dis.csv')\n",
    "    dis = df_dis.values[:,1:]\n",
    "    beta = 1.5\n",
    "    def weight(x,beta=beta):\n",
    "        return np.log((1+max_data**beta)/(1+x**beta))\n",
    "    df_dis = pd.DataFrame(dis)\n",
    "    max_data = df_dis.max().max()\n",
    "    df_wdis = df_dis.applymap(weight)\n",
    "    mx = df_wdis.values\n",
    "    def normalize(matx):\n",
    "        rowsum = np.array(matx.sum(1))\n",
    "        r_inv = np.power(rowsum, -1).flatten()\n",
    "        r_inv[np.isinf(r_inv)] = 0.\n",
    "        r_mat_inv = np.diag(r_inv)\n",
    "        matx = r_mat_inv.dot(matx)\n",
    "        return matx\n",
    "    mx = normalize(mx)\n",
    "    np.save(data_dir+'final_input/wdis_normalize.npy',mx)\n",
    "else:\n",
    "    dis = np.load(data_dir+'final_input/wdis_normalize.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not adj_exist:\n",
    "    ls_edge = []\n",
    "    for _,row in gdf_unit.iterrows():\n",
    "        neighbors = gdf_unit[~gdf_unit.geometry.disjoint(row.geometry)]['idx'].tolist()\n",
    "        for i in neighbors:\n",
    "            if row['idx']<i:\n",
    "                a = [row['idx'],i,1]\n",
    "                ls_edge.append(a)\n",
    "    with open(data_dir+'final_input/ls_edge.pkl', 'wb') as f:\n",
    "        pickle.dump(ls_edge, f) \n",
    "    adj = np.zeros(dis.shape)\n",
    "    for i in ls_edge:\n",
    "        adj[i[0],i[1]] = 1\n",
    "        adj[i[1],i[0]] = 1\n",
    "    adj = adj + np.eye(adj.shape[0])\n",
    "    np.save(data_dir+'final_input/adj.npy',adj)\n",
    "else:\n",
    "    adj = np.load(data_dir+'final_input/adj.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary (input - interaction - output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ⅰ - input (property - Doc2Vec Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12,345 * 72\n",
    "# df_zonevec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ⅱ - interaction (flow/distance-decay/adjacency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flow (12,345, 12,345)\n",
    "# flow_mx\n",
    "# Distance-decay (12,345, 12,345)\n",
    "# dis\n",
    "# Adjacency (12,345,12,345)\n",
    "# adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ⅲ - output (urban function - LU area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Land use (12,345,8)\n",
    "# df_area"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GCN",
   "language": "python",
   "name": "gcn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
